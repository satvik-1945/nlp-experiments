# ğŸ” Word2Vec CBOW: Real vs. Fake Embedding Implementations

This repository demonstrates and debunks a common myth in the NLP community:  
> "CBOW with One-Hot input vectors behaves just like real Word2Vec."

We implement and compare:
1. ğŸ§  **Fake CBOW** â€” using concatenated one-hot vectors as input (a widespread misunderstanding)
2. ğŸ’¡ **Real CBOW** â€” using an embedding lookup table that is learned during training (true Word2Vec-style)

We also visualize the learned word clusters to show why proper embeddings are critical for meaningful vector space representations.

---

## ğŸ“ Dataset

A simple, naive dataset consisting of 3 semantic clusters:
- ğŸŸ¥ Fruits: `apple`, `banana`, `mango`, `orange`, ...
- ğŸŸ¦ Countries: `india`, `china`, `france`, ...
- ğŸŸ© Music Genres: `jazz`, `rock`, `pop`, `classical`, ...

Stop words have been removed for clarity and stronger signal during training.

---

## ğŸ§ª CBOW Variants

### âœ… Fake CBOW (One-Hot Concatenation)

- Context words are converted to one-hot vectors
- Concatenated into a large sparse input layer
- Connected to a hidden layer â†’ output vocab distribution
- Very inefficient and **not representative of Word2Vec**

### âœ… Real CBOW (Embedding Lookup)

- Context word indices are used to look up dense vectors
- Embeddings are averaged and passed through a linear layer
- Softmax + Cross-Entropy used for prediction
- Embedding matrix gets updated via backpropagation

---

## ğŸ” Training

Both models are trained using:
- Optimizer: Vanilla SGD
- Loss: Negative Log-Likelihood (Cross Entropy)
- Context window: 2

---

## ğŸ“Š Visualization

We reduce the learned embeddings to 2D using **t-SNE** or **PCA**, and plot them using Matplotlib.
